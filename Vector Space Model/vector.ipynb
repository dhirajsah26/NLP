{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "nltk.download('punkt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/capitals.txt\", delimiter = ' ')\n",
    "data.columns = ['city1', 'country1', 'city2' , 'country2']\n",
    "# print first five elements in the department \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4951, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "f = open('data/capitals.txt','r').read()\n",
    "set_words = set(nltk.word_tokenize(f))\n",
    "select_words = words = ['King', 'queen', 'oil', 'gas', 'happy', 'sad' , 'city' , 'town', 'village', 'country', 'continient', 'petroleum', 'joyful']\n",
    "\n",
    "for w in select_words:\n",
    "    set_words.add(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "def get_word_embeddings(embeddings):\n",
    "    word_embeddings = {}\n",
    "    for word in embeddings.key_to_index:\n",
    "        if word in set_words:\n",
    "            word_embeddings[word] = embeddings[word]\n",
    "    return word_embeddings\n",
    "    \n",
    "# testing the function \n",
    "word_embeddings = get_word_embeddings(embeddings)\n",
    "print(len(word_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the word embeddings as a python dictionary. As stated these have already been obtained through a machine learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = pickle.load(open(\"data/word_embeddings_subset.p\", \"rb\"))\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the word embedding is a 300 dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will write a function that will use the word embeddings topredict relationships among words. \n",
    "\n",
    "The input will take as input three words.\n",
    "The first two are related to each other. \n",
    "It will predict a 4th word which is related to third word in a similar manner as the two first two words are related to each other. \n",
    "As an example,\"Athens is to Greece as Bangkok is to __\"?\n",
    "You wiull write a program that is capable of finding the fourth word. \n",
    "We will give you a hint to show how to compute this. \n",
    "\n",
    "A smimilar analogy would be the following:\n",
    "IN chess:\n",
    "king - man + woman = queen \n",
    "\n",
    "\n",
    "The cosine Similarity function is:\n",
    "cos(theta)A.B/||A||||B|| = sumation of (prodcut) of AB)/ root of sqaure A product squatre  root of B \n",
    "\n",
    "A and B represent the word vectors and A or B represent index I of the vector. Note that if A and B are identical, you will get cos(theta) = 1. \n",
    "\n",
    "otherwise if they are the opposite, meainng, A = -B, then you would get cos(theta) = -1\n",
    "if you get cos(theta) = 0 , that means that they are orthogonal (or perpendicular). \n",
    "Numbers between 0 and 1 indicates a similarity score. \n",
    "Numbers between -1 and 0 indicates a dissimilarity score. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1 Cosine_Similarity\n",
    "\n",
    "implement a function that takes in two word vectors and computes the cosine distance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    '''Input:\n",
    "            A: a numpy array which corresponds to a word vector\n",
    "            B: A numpy array which correcsponds to a word vector\n",
    "        Output:\n",
    "            cos: numerical number representing the cosine similariy between A and . \n",
    "    '''\n",
    "\n",
    "    ### Start COde Here ### \n",
    "    dot = np.dot(A,B)\n",
    "    norma = np.linalg.norm(A)  \n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot/(norma*normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510956"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "\n",
    "cosine_similarity(king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance \n",
    "You will now implkement a function that computes the suiimilarity between two vectors using the Euclidean distanc. Euclidean distance is defined as :\n",
    "\n",
    "d(A,B) = d(B,A) = root of square of (A-B)^2 + (A-B)^2...+(A - B) ^ 2\n",
    "\n",
    "n i sthe number of elements in the vector \n",
    "A and B are the corresponding word vectors. \n",
    "The more similar the words, the more likley the Euclidean distance will be close to 0. \n",
    "\n",
    "# Euclidean\n",
    "Implement a function that computesthe Euclidean distance between two vectors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(A, B):\n",
    "    '''' INput:\n",
    "            A: a numpy array which corresponds to a word vector\n",
    "            B: a numpy aray which correspojnse to a wprd vector\n",
    "        Output:\n",
    "        d: numerical number representing the Euclidean distance between A and B. \n",
    "    '''\n",
    "\n",
    "    d = np.sqrt(np.sum((A-B)**2))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4796925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function\n",
    "euclidean(king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 FInding the country of each Capital \n",
    "Now, you will use the previous  functions to compute similarities between vectors, and use these to find the capital cities of countries. You will write a function that takes in three word, and the embeding dictionary. Your task is to find the capital citiesr. FOr examples, given the following words:\n",
    "\n",
    "1 Athens 2: Grece 3: Baghdad, \n",
    "\n",
    "Your taks is to predict the country4: Iraq.\n",
    "\n",
    "# Exercise 3 - get_country\n",
    "\n",
    "### Instructions:\n",
    "1. TO predict the capital you might want to look at the King - Man + Woman = Queen exampleabove, and implement that scheme into a mathematical functiona, using the word embeddings and a similarity function. \n",
    "\n",
    "2. Iterate over the embedings dictionary and compute the cosine similarity score between your vector abd the current word embedding. \n",
    "3. You should add a check to make sure that the word you return is not any of the words that you fed in your function. Return the one with the highest score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings, cosine_similarity = cosine_similarity):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # storing the city1 country1 and city2 in a set called group\n",
    "    group = [city1, country1, city2]\n",
    "\n",
    "    # get embeddings of the city1\n",
    "    city1_emb = embeddings[city1]\n",
    "\n",
    "    # get_embedding of country 1\n",
    "    country1_emb = embeddings[country1]\n",
    "\n",
    "    #get embedding of city2\n",
    "    city2_emb = embeddings[city2]\n",
    "\n",
    "    # get embedding of country2 \n",
    "    vec = (country1_emb - city1_emb) + city2_emb\n",
    "\n",
    "    # Intialize the similarity to -1 (it will be replaced by  a smilarities that aree closer to +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # intialize country to empty string\n",
    "    country = ''\n",
    "\n",
    "    # Loop through all words in the embeddings dictionary\n",
    "    for word in embeddings.keys():\n",
    "        #first check if the word is in group\n",
    "        if word not in group:\n",
    "             # get the word embedding\n",
    "             word_emb = embeddings[word]\n",
    "\n",
    "             # calculate the cosine similarity \n",
    "             cur_similarity = cosine_similarity( vec , word_emb)\n",
    "\n",
    "             if cur_similarity> similarity:\n",
    "                 similarity = cur_similarity\n",
    "                 country = (word, similarity)\n",
    "    return country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Egypt', 0.7626821)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the model accuracy of the new function on the dataset\n",
    "\n",
    "Accuracy = correct # of predictions/ Toral # of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data, get_country = get_country):\n",
    "    ''''''\n",
    "    #initialize num correct to zero\n",
    "    num_correct = 0\n",
    "    # Loop through the rows of the dataframe\n",
    "    for i , row in data.iterrows():\n",
    "        city1 = row['city1']\n",
    "\n",
    "        country1 = row['country1']\n",
    "\n",
    "        city2 = row['city2']\n",
    "\n",
    "        country2 = row['country2']\n",
    "\n",
    "        predcited_country2,_ = get_country(city1,country1, city2, word_embeddings )\n",
    "\n",
    "        if predcited_country2 == country2:\n",
    "            # increment the number of correct by 1 \n",
    "            num_correct +=1\n",
    "\n",
    "    m = len(data)\n",
    "    \n",
    "    #calculate the accuracy by dividing the number correct by m\n",
    "    accuracy = num_correct/m\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(words, embedding):\n",
    "    X = np.zeros((1,300))\n",
    "    for word in words:\n",
    "        if word in word_embeddings:\n",
    "            x = embedding[word]\n",
    "            X = np.row_stack((X,x))\n",
    "    X = X[1:,:]\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = get_vectors(select_words,word_embeddings)\n",
    "type(vec) , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X,n_components = 2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Mean Centerd the data\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "    centered_data  = X - X_mean\n",
    "\n",
    "    # Calculate Covariance matrix\n",
    "    cov_matrix = np.cov(centered_data.T)\n",
    "\n",
    "    # print(len(cov_matrix))\n",
    "    \n",
    "\n",
    "\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Sort eigenvalue in increasing order (get the indices from the sort)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "\n",
    "    print(idx_sorted)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    print(eigen_vals)\n",
    "    print(\"---------------------\")\n",
    "    print(eigen_vecs)\n",
    "    # reverse the order so that it's from highest to lowest.\n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "    print(\"---------------------\")\n",
    "    print(idx_sorted_decreasing)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # sort the eigen values by idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "\n",
    "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    eigen_vecs_sorted = eigen_vecs[idx_sorted_decreasing]\n",
    "\n",
    "    # select the first n eigenvectors \n",
    "    # (n is desired dimension of rescaled data array, or n_components)\n",
    "\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,:n_components]\n",
    "\n",
    "    X_reduced = np.dot(eigen_vecs_subset.T,centered_data.T)\n",
    "\n",
    "    X_reduced = X_reduced.T\n",
    "\n",
    "\n",
    "\n",
    "    return X_reduced\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 5 4 8 9 7 6 2 1]\n",
      "---------------------\n",
      "[-5.55111512e-17+0.00000000e+00j  5.48501886e-01+0.00000000e+00j\n",
      "  2.50881048e-01+0.00000000e+00j -1.78861952e-17+0.00000000e+00j\n",
      " -1.05623286e-17+7.71792418e-18j -1.05623286e-17-7.71792418e-18j\n",
      "  7.81747807e-18+6.45286014e-18j  7.81747807e-18-6.45286014e-18j\n",
      " -2.81696942e-18+0.00000000e+00j  5.20502801e-19+0.00000000e+00j]\n",
      "---------------------\n",
      "[[-9.54473629e-01+0.j          2.98289378e-01+0.j\n",
      "  -1.88121436e-03+0.j          6.20046919e-03+0.j\n",
      "  -4.39195956e-02-0.07436004j -4.39195956e-02+0.07436004j\n",
      "  -1.31785887e-02-0.06563523j -1.31785887e-02+0.06563523j\n",
      "  -1.74129395e-02+0.j          6.08961426e-02+0.j        ]\n",
      " [ 6.46841109e-02+0.j          2.06739077e-01+0.j\n",
      "  -3.78517564e-02+0.j         -1.13742091e-01+0.j\n",
      "   1.06076211e-01-0.04403154j  1.06076211e-01+0.04403154j\n",
      "  -1.48964985e-01-0.32115878j -1.48964985e-01+0.32115878j\n",
      "  -5.02480557e-02+0.j         -1.76582172e-02+0.j        ]\n",
      " [ 5.14355546e-02+0.j          1.65857022e-01+0.j\n",
      "   2.01735451e-01+0.j         -2.11567908e-01+0.j\n",
      "  -5.52506210e-01+0.j         -5.52506210e-01-0.j\n",
      "  -5.41725191e-04+0.09693839j -5.41725191e-04-0.09693839j\n",
      "   3.74535954e-02+0.j          1.69459136e-01+0.j        ]\n",
      " [ 2.48574182e-02+0.j          8.31573118e-02+0.j\n",
      "   5.73668120e-01+0.j          6.35681668e-01+0.j\n",
      "   8.34899290e-02-0.16803658j  8.34899290e-02+0.16803658j\n",
      "   1.22501728e-01+0.11536117j  1.22501728e-01-0.11536117j\n",
      "  -3.42165865e-01+0.j          2.70864795e-01+0.j        ]\n",
      " [ 1.92310429e-01+0.j          6.14555453e-01+0.j\n",
      "  -1.27507609e-01+0.j         -3.80778593e-01+0.j\n",
      "   3.02194175e-01-0.19043121j  3.02194175e-01+0.19043121j\n",
      "   2.03963573e-01+0.39349727j  2.03963573e-01-0.39349727j\n",
      "   6.12937695e-02+0.j         -6.48960605e-02+0.j        ]\n",
      " [ 1.25017934e-01+0.j          4.03640689e-01+0.j\n",
      "   5.71656816e-01+0.j         -2.02068891e-01+0.j\n",
      "   3.14085487e-02+0.35910517j  3.14085487e-02-0.35910517j\n",
      "  -4.61579371e-02-0.2124561j  -4.61579371e-02+0.2124561j\n",
      "  -5.24603806e-02+0.j         -1.43799865e-01+0.j        ]\n",
      " [-5.28123960e-02+0.j         -1.67521425e-01+0.j\n",
      "   2.32922772e-01+0.j         -2.29314405e-01+0.j\n",
      "  -2.13020619e-01+0.04066251j -2.13020619e-01-0.04066251j\n",
      "   1.10112949e-01+0.24339499j  1.10112949e-01-0.24339499j\n",
      "  -2.63928520e-01+0.j          6.72747093e-01+0.j        ]\n",
      " [-1.00645623e-01+0.j         -3.20679030e-01+0.j\n",
      "   2.17117497e-01+0.j         -3.12509508e-01+0.j\n",
      "  -1.61742586e-02-0.03625707j -1.61742586e-02+0.03625707j\n",
      "   5.73544697e-01+0.j          5.73544697e-01-0.j\n",
      "   2.23809471e-01+0.j         -3.32507269e-01+0.j        ]\n",
      " [-2.40506450e-02+0.j         -7.85652485e-02+0.j\n",
      "  -2.54874036e-01+0.j         -6.19213952e-02+0.j\n",
      "   5.08773943e-03+0.38826568j  5.08773943e-03-0.38826568j\n",
      "   2.11564419e-01+0.09629664j  2.11564419e-01-0.09629664j\n",
      "  -8.66074124e-01+0.j          5.15318479e-01+0.j        ]\n",
      " [ 1.24187928e-01+0.j          3.95200626e-01+0.j\n",
      "  -3.45496594e-01+0.j          4.45271160e-01+0.j\n",
      "  -4.12450010e-01+0.10887521j -4.12450010e-01-0.10887521j\n",
      "   3.46423523e-01-0.1200079j   3.46423523e-01+0.1200079j\n",
      "  -4.84698102e-02+0.j          2.00755926e-01+0.j        ]]\n",
      "---------------------\n",
      "[1 2 6 7 9 8 4 5 3 0]\n",
      "---------------------\n",
      "Your Original Matrix was (3, 10) and it became: \n",
      "[[ 0.01447897+0.j  0.04904179+0.j]\n",
      " [ 0.30525158+0.j -0.16273421+0.j]\n",
      " [-0.31973055+0.j  0.11369242+0.j]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.rand(3,10)\n",
    "X_reduced = compute_pca(X, n_components = 2)\n",
    "print(\"Your Original Matrix was \"+ str(X.shape) + \" and it became: \")\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_vectors(words,word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compute_pca(X,2)\n",
    "plt.scatter(result[:,0], result[:,1])\n",
    "for i , word in enumerate(words):\n",
    "    plt.annotate(word, xy= (result[i,0] - 0.05, result[i,1]+0.1))\n",
    "\n",
    "plt .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_pca(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the dataset X and reduce \n",
    "    its dimensionality to n_components.\n",
    "    \n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        The input data matrix, where rows represent samples and columns represent features.\n",
    "    n_components : int\n",
    "        The number of principal components to keep.\n",
    "    \n",
    "    Returns:\n",
    "    X_reduced : numpy.ndarray\n",
    "        The data projected into the lower-dimensional space.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Mean Center the data\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    centered_data = X - X_mean\n",
    "\n",
    "    # Step 2: Calculate Covariance matrix\n",
    "    cov_matrix = np.cov(centered_data.T)\n",
    "\n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Step 4: Sort eigenvalues and corresponding eigenvectors in decreasing order\n",
    "    idx_sorted_decreasing = np.argsort(eigen_vals)[::-1]\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "    angle = eigen_vals_sorted[:,]\n",
    "    eigen_vecs_sorted = eigen_vecs[:, idx_sorted_decreasing]  # Corrected: Sort column-wise\n",
    "\n",
    "    # Step 5: Select the top n_components eigenvectors\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:, :n_components]\n",
    "\n",
    "    # Step 6: Transform the data (project onto the new subspace)\n",
    "    X_reduced = np.dot(centered_data, eigen_vecs_subset)\n",
    "\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30127926 -0.07153665]\n",
      " [ 0.02356571  0.10795368]\n",
      " [-0.46979427 -0.29834505]\n",
      " [-0.5365572   0.26181762]\n",
      " [-0.07179863  0.58271646]\n",
      " [ 0.16174147  0.45345974]\n",
      " [-0.10338027  0.15144529]\n",
      " [-0.00308307 -0.23848444]\n",
      " [ 0.18731793 -0.1860031 ]\n",
      " [-0.08660801  0.1950965 ]\n",
      " [-0.28353898 -0.34127674]\n",
      " [ 0.12252711 -0.63472579]\n",
      " [-0.29860216  0.24181588]\n",
      " [-0.22344147  0.33880445]\n",
      " [ 0.28164284 -0.08917147]\n",
      " [ 0.71590282 -0.47006415]\n",
      " [ 0.15256182 -0.17955227]\n",
      " [-0.39516275 -0.52608758]\n",
      " [ 0.63920177  0.68245941]\n",
      " [-0.18177197 -0.08518042]\n",
      " [-0.22862541  0.12132878]\n",
      " [ 0.021786    0.07261526]\n",
      " [-0.04334624  0.39070019]\n",
      " [ 0.03698006  0.06548567]\n",
      " [-0.43569335  0.36210103]\n",
      " [ 0.48105184 -0.44098804]\n",
      " [-0.08122268 -0.07021245]\n",
      " [ 0.49848873  0.23528403]\n",
      " [-0.29827854  0.12008631]\n",
      " [-0.29185762  0.18139257]\n",
      " [ 0.04318283 -0.07146148]\n",
      " [ 0.46771932 -0.31894354]\n",
      " [ 0.69034537 -0.15355969]\n",
      " [ 0.58746729 -0.18832098]\n",
      " [-0.12961854 -0.62156883]\n",
      " [ 0.2492036  -0.22562629]\n",
      " [-0.59119221  0.44498042]\n",
      " [ 0.16607548 -0.46049068]\n",
      " [ 0.13997622  0.28280342]\n",
      " [ 0.18153712 -0.6853527 ]\n",
      " [-0.01962461 -0.08159191]\n",
      " [ 0.55998378 -0.12503203]\n",
      " [-0.10647328  0.2046943 ]\n",
      " [ 0.6145353  -0.13150589]\n",
      " [-0.13828491  0.18398618]\n",
      " [-0.06094703 -0.2048024 ]\n",
      " [-0.04454606 -0.41198671]\n",
      " [-0.14371395  0.18813731]\n",
      " [ 0.46367824 -0.00827913]\n",
      " [ 0.37923108 -0.02181525]\n",
      " [-0.52336563 -0.23829244]\n",
      " [ 0.10046877 -0.46238098]\n",
      " [-0.28183642 -0.04368487]\n",
      " [ 0.04552222  0.3092098 ]\n",
      " [-0.50596988 -0.34515547]\n",
      " [ 0.03578456  0.13728764]\n",
      " [-0.60940676  0.06864143]\n",
      " [ 0.01297845  0.28952447]\n",
      " [-0.10103446  0.38973413]\n",
      " [ 0.40013646 -0.10951375]\n",
      " [ 0.0543931   0.38466398]\n",
      " [ 0.33142777 -0.11027328]\n",
      " [-0.57001643 -0.59096984]\n",
      " [-0.39580995 -0.30161101]\n",
      " [-0.16629058  0.08431402]\n",
      " [ 0.14724248 -0.00149041]\n",
      " [ 0.64490216 -0.17798437]\n",
      " [-0.59288019 -0.12086533]\n",
      " [ 0.14967556  0.17156892]\n",
      " [-0.38279252 -0.12244065]\n",
      " [-0.1877708  -0.15249881]\n",
      " [-0.17399982  0.36886268]\n",
      " [-0.16315794  0.51612559]\n",
      " [ 0.09142502 -0.39835776]\n",
      " [-0.31428882 -0.08725809]\n",
      " [ 0.05297651  0.03030149]\n",
      " [ 0.19102454 -0.04566818]\n",
      " [-0.10272692 -0.0024349 ]\n",
      " [ 0.31546371  0.69586721]\n",
      " [ 0.60797785  0.09590346]\n",
      " [ 0.24827559  0.03771252]\n",
      " [-0.48297227  0.23752676]\n",
      " [ 0.59196494  0.08789273]\n",
      " [-0.50312896 -0.36259947]\n",
      " [ 0.19249969 -0.29091453]\n",
      " [-0.56283455  0.0315032 ]\n",
      " [-0.00213275  0.56360912]\n",
      " [-0.43055424  0.2570929 ]\n",
      " [ 0.31100376 -0.24321804]\n",
      " [-0.07900768  0.24986133]\n",
      " [-0.22281492 -0.01336861]\n",
      " [ 0.03512066  0.1782173 ]\n",
      " [-0.40995133  0.14991437]\n",
      " [-0.4865043  -0.67427268]\n",
      " [ 0.32711537  0.4984424 ]\n",
      " [-0.34969579  0.16429456]\n",
      " [ 0.17051775  0.33589042]\n",
      " [ 0.29806192  0.16049107]\n",
      " [ 0.02220518 -0.69209251]\n",
      " [ 0.32296011  0.56569364]]\n"
     ]
    }
   ],
   "source": [
    "X_sample = np.random.rand(100, 5)  # 100 samples, 5 features\n",
    "X_reduced = compute_pca(X_sample, n_components=2)\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_open3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
